{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as cv\n",
    "import sklearn.ensemble        as ensem\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = make_classification(n_samples            = 5000, \n",
    "                                   n_features           = 50,\n",
    "                                   n_informative        = 20,\n",
    "                                   n_classes            = 2,\n",
    "                                   n_clusters_per_class = 10,\n",
    "                                   random_state         = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = cv.train_test_split(X_all,\n",
    "                                           y_all, \n",
    "                                           random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierCV(ensem.RandomForestClassifier):\n",
    "    '''\n",
    "        This class implements cross validation *ONLY* for the \n",
    "        number of estimators. It does this using OOB scoring and\n",
    "        warm starts. In theory this should be doable in sk-learn by\n",
    "        default, but I am unable to make it work without this\n",
    "        roll-you-own solution\n",
    "        \n",
    "        Note: always computes up to max_estimators. Then picks the\n",
    "        smallest possible within the tolerance. There are presumably\n",
    "        better ways to detect stabilization.\n",
    "        \n",
    "        For other hyperparemeters, use GridSearch, below\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, max_estimators = 500, tolerance = 0.005, **kwargs):\n",
    "        kwargs['warm_start']   = True\n",
    "        kwargs['oob_score']    = True\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_estimators = max_estimators\n",
    "        self.tolerance      = tolerance\n",
    "        self.use_oob_score  = True\n",
    "        self.init_kwargs    = kwargs\n",
    "        \n",
    "    def reset_and_clone(self):\n",
    "        return RandomForestClassifierCV(max_estimators = self.max_estimators,\n",
    "                                        tolerance      = self.tolerance, \n",
    "                                       **self.init_kwargs)\n",
    "        \n",
    "    def score(self, *args, **kwargs):\n",
    "        if self.use_oob_score:\n",
    "            return self.best_score\n",
    "        else:\n",
    "            return super().score(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_est_scores_ = {}\n",
    "        for i in range(100, self.max_estimators + 1, 100):\n",
    "            self.n_estimators = i\n",
    "            super().fit(X, y)\n",
    "            self.n_est_scores_[i] = self.oob_score_\n",
    "            \n",
    "        self.best_score = max(self.n_est_scores_.values())\n",
    "        self.best_n     = next(n for n, v in self.n_est_scores_.items() if v > self.best_score - self.tolerance)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def finalize(self, X, y):\n",
    "        self.use_oob_score = False\n",
    "        \n",
    "        self.set_params(warm_start = False)\n",
    "        \n",
    "        self.n_estimators = self.best_n\n",
    "        super().fit(X, y)\n",
    "        \n",
    "    def get_params(self, *args, **kwargs):\n",
    "        out = ensem.RandomForestClassifier().get_params()\n",
    "        out['max_estimators'] = self.max_estimators\n",
    "        out['tolerance']      = self.tolerance\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch():\n",
    "    '''\n",
    "        Grid Search *without* cross validation.\n",
    "        Doesn't implement full feature set of GridSearch\n",
    "        \n",
    "        Just fit, predict in a bare-bones fashion\n",
    "        \n",
    "        Designed for use with RandomForestCV above\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, estimator, param_grid, verbose = 0):\n",
    "        self.param_grid       = cv.ParameterGrid(param_grid)\n",
    "        self.parent_estimator = estimator\n",
    "        self.verbose          = verbose\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.best_params = None\n",
    "        self.best_model  = None\n",
    "        self.best_score  = 0\n",
    "        \n",
    "        progress_count   = 0\n",
    "        if self.verbose:\n",
    "            print(f'{len(self.param_grid)} Iterations to do')\n",
    "        \n",
    "        for param_dict in self.param_grid:\n",
    "            model = self.parent_estimator.reset_and_clone()\n",
    "            model.set_params(**param_dict)\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            if model.score() > self.best_score:\n",
    "                self.best_params = param_dict\n",
    "                self.best_model  = model\n",
    "                self.best_score  = model.score()\n",
    "                \n",
    "            progress_count += 1\n",
    "            if self.verbose and progress_count % 20 == 0:\n",
    "                print(f'{progress_count} Done')\n",
    "        \n",
    "        # This fails if every model somehow had an accuracy of 0...\n",
    "        assert bool(self.best_params)\n",
    "        \n",
    "        self.best_model.finalize(X, y)\n",
    "        \n",
    "        return self.best_model\n",
    "        \n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.best_model.predict(*args, **kwargs)\n",
    "    \n",
    "    def score(self, *args, **kwargs):\n",
    "        return self.best_model.score(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion'         : ['entropy'],\n",
    "    'max_depth'         : [None, 1, 3],\n",
    "    'min_samples_leaf'  : [1, 5 ],\n",
    "    'max_features'      : [5, 'sqrt'],\n",
    "    'max_leaf_nodes'    : [None, 10, 50],\n",
    "    'class_weight'      : [None]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = GridSearch(RandomForestClassifierCV(), param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 45s, sys: 1.4 s, total: 3min 46s\n",
      "Wall time: 3min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifierCV(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, max_samples=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                         n_jobs=None, oob_score=False, random_state=None,\n",
       "                         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.708"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synovium",
   "language": "python",
   "name": "synovium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
